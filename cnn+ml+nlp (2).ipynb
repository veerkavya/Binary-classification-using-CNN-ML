{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1VpIC5MN2o8",
        "outputId": "f32d3bf2-5aa5-418c-8844-5f5651e865d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "hE8uZXiUMv4K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n"
      ],
      "metadata": {
        "id": "b2Cg99nqzNR6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OTV8kallEW17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **single layer model with trainable = false**"
      ],
      "metadata": {
        "id": "YcDvu9zBEzmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ModelCheckpoint callback to save the model in .keras format\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoint.keras\",\n",
        "                                                         save_best_only=True,\n",
        "                                                         save_weights_only=False)\n",
        "\n",
        "# EarlyStopping callback to stop training when validation accuracy stops improving\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "9mqMxi1xOPZU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agvH5Ry0xr3U",
        "outputId": "30f7ef16-ac54-496a-f785-31efe39ba598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Training samples: 8635, Validation samples: 960, Test samples: 1067\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.5402 - loss: 0.6953 - val_accuracy: 0.5750 - val_loss: 0.6804\n",
            "Epoch 2/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5637 - loss: 0.6811 - val_accuracy: 0.5531 - val_loss: 0.6825\n",
            "Epoch 3/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5646 - loss: 0.6814 - val_accuracy: 0.5417 - val_loss: 0.6938\n",
            "Epoch 4/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5628 - loss: 0.6770 - val_accuracy: 0.5552 - val_loss: 0.6830\n",
            "Epoch 5/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5816 - loss: 0.6743 - val_accuracy: 0.5792 - val_loss: 0.6742\n",
            "Epoch 6/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5637 - loss: 0.6816 - val_accuracy: 0.5479 - val_loss: 0.6892\n",
            "Epoch 7/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5917 - loss: 0.6706 - val_accuracy: 0.5573 - val_loss: 0.6798\n",
            "Epoch 8/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5929 - loss: 0.6687 - val_accuracy: 0.5833 - val_loss: 0.6729\n",
            "Epoch 9/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6053 - loss: 0.6646 - val_accuracy: 0.5604 - val_loss: 0.6802\n",
            "Epoch 10/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5948 - loss: 0.6635 - val_accuracy: 0.5667 - val_loss: 0.6812\n",
            "Epoch 11/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5986 - loss: 0.6591 - val_accuracy: 0.5979 - val_loss: 0.6701\n",
            "Epoch 12/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6076 - loss: 0.6589 - val_accuracy: 0.5813 - val_loss: 0.6743\n",
            "Epoch 13/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5987 - loss: 0.6616 - val_accuracy: 0.5865 - val_loss: 0.6702\n",
            "Epoch 14/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6156 - loss: 0.6561 - val_accuracy: 0.5729 - val_loss: 0.6743\n",
            "Epoch 15/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6179 - loss: 0.6527 - val_accuracy: 0.5760 - val_loss: 0.6794\n",
            "Epoch 16/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6127 - loss: 0.6552 - val_accuracy: 0.5750 - val_loss: 0.6729\n",
            "Epoch 17/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6244 - loss: 0.6475 - val_accuracy: 0.5688 - val_loss: 0.6840\n",
            "Epoch 18/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6058 - loss: 0.6583 - val_accuracy: 0.5969 - val_loss: 0.6710\n",
            "Epoch 19/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6330 - loss: 0.6431 - val_accuracy: 0.5906 - val_loss: 0.6765\n",
            "Epoch 20/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6235 - loss: 0.6449 - val_accuracy: 0.5854 - val_loss: 0.6719\n",
            "Epoch 21/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6224 - loss: 0.6468 - val_accuracy: 0.5813 - val_loss: 0.6771\n",
            "Epoch 22/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6359 - loss: 0.6408 - val_accuracy: 0.5552 - val_loss: 0.7024\n",
            "Epoch 23/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6247 - loss: 0.6449 - val_accuracy: 0.5802 - val_loss: 0.6821\n",
            "Epoch 24/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6347 - loss: 0.6413 - val_accuracy: 0.5833 - val_loss: 0.6730\n",
            "Epoch 25/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6408 - loss: 0.6347 - val_accuracy: 0.5896 - val_loss: 0.6721\n",
            "Epoch 26/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6468 - loss: 0.6317 - val_accuracy: 0.5792 - val_loss: 0.6744\n",
            "Epoch 27/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6393 - loss: 0.6304 - val_accuracy: 0.5854 - val_loss: 0.6753\n",
            "Epoch 28/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6480 - loss: 0.6297 - val_accuracy: 0.5844 - val_loss: 0.6859\n",
            "Epoch 29/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6228 - loss: 0.6423 - val_accuracy: 0.5813 - val_loss: 0.6730\n",
            "Epoch 30/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6343 - loss: 0.6369 - val_accuracy: 0.5958 - val_loss: 0.6719\n",
            "Epoch 31/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6444 - loss: 0.6303 - val_accuracy: 0.5531 - val_loss: 0.7105\n",
            "Epoch 32/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6391 - loss: 0.6341 - val_accuracy: 0.5927 - val_loss: 0.6718\n",
            "Epoch 33/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6488 - loss: 0.6224 - val_accuracy: 0.5885 - val_loss: 0.6823\n",
            "Epoch 34/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6538 - loss: 0.6210 - val_accuracy: 0.5760 - val_loss: 0.6775\n",
            "Epoch 35/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6429 - loss: 0.6273 - val_accuracy: 0.5927 - val_loss: 0.6775\n",
            "Epoch 36/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6441 - loss: 0.6285 - val_accuracy: 0.5542 - val_loss: 0.6954\n",
            "Epoch 37/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6479 - loss: 0.6232 - val_accuracy: 0.5802 - val_loss: 0.6876\n",
            "Epoch 38/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6560 - loss: 0.6210 - val_accuracy: 0.5740 - val_loss: 0.6826\n",
            "Epoch 39/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6560 - loss: 0.6166 - val_accuracy: 0.5656 - val_loss: 0.7138\n",
            "Epoch 40/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6588 - loss: 0.6194 - val_accuracy: 0.5865 - val_loss: 0.6825\n",
            "Epoch 41/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6622 - loss: 0.6155 - val_accuracy: 0.5823 - val_loss: 0.6831\n",
            "Epoch 42/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6507 - loss: 0.6208 - val_accuracy: 0.5667 - val_loss: 0.6865\n",
            "Epoch 43/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6603 - loss: 0.6190 - val_accuracy: 0.5750 - val_loss: 0.6839\n",
            "Epoch 44/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6630 - loss: 0.6096 - val_accuracy: 0.5740 - val_loss: 0.6947\n",
            "Epoch 45/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6678 - loss: 0.6088 - val_accuracy: 0.5823 - val_loss: 0.6835\n",
            "Epoch 46/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6676 - loss: 0.6116 - val_accuracy: 0.5823 - val_loss: 0.6792\n",
            "Epoch 47/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6611 - loss: 0.6145 - val_accuracy: 0.5938 - val_loss: 0.6809\n",
            "Epoch 48/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6707 - loss: 0.6041 - val_accuracy: 0.5760 - val_loss: 0.6832\n",
            "Epoch 49/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6765 - loss: 0.6045 - val_accuracy: 0.5479 - val_loss: 0.7257\n",
            "Epoch 50/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6449 - loss: 0.6233 - val_accuracy: 0.5802 - val_loss: 0.6900\n",
            "Epoch 51/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6671 - loss: 0.6104 - val_accuracy: 0.5854 - val_loss: 0.6821\n",
            "Epoch 52/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6681 - loss: 0.6077 - val_accuracy: 0.5719 - val_loss: 0.6903\n",
            "Epoch 53/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6681 - loss: 0.6051 - val_accuracy: 0.5656 - val_loss: 0.6877\n",
            "Epoch 54/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6683 - loss: 0.6003 - val_accuracy: 0.5729 - val_loss: 0.6974\n",
            "Epoch 55/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 0.5967 - val_accuracy: 0.5823 - val_loss: 0.6883\n",
            "Epoch 56/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6734 - loss: 0.5990 - val_accuracy: 0.5771 - val_loss: 0.6856\n",
            "Epoch 57/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6799 - loss: 0.5939 - val_accuracy: 0.5896 - val_loss: 0.6899\n",
            "Epoch 58/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6774 - loss: 0.5925 - val_accuracy: 0.5615 - val_loss: 0.7248\n",
            "Epoch 59/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6759 - loss: 0.6037 - val_accuracy: 0.5646 - val_loss: 0.6910\n",
            "Epoch 60/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6741 - loss: 0.5973 - val_accuracy: 0.5688 - val_loss: 0.6907\n",
            "Epoch 61/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6868 - loss: 0.5878 - val_accuracy: 0.5760 - val_loss: 0.7116\n",
            "Epoch 62/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6515 - loss: 0.6208 - val_accuracy: 0.5885 - val_loss: 0.6855\n",
            "Epoch 63/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6617 - loss: 0.6093 - val_accuracy: 0.5698 - val_loss: 0.6982\n",
            "Epoch 64/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6720 - loss: 0.6012 - val_accuracy: 0.5667 - val_loss: 0.6919\n",
            "Epoch 65/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6770 - loss: 0.5986 - val_accuracy: 0.5521 - val_loss: 0.7360\n",
            "Epoch 66/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6878 - loss: 0.5901 - val_accuracy: 0.5729 - val_loss: 0.6941\n",
            "Epoch 67/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6898 - loss: 0.5887 - val_accuracy: 0.5813 - val_loss: 0.7151\n",
            "Epoch 68/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6900 - loss: 0.5849 - val_accuracy: 0.5667 - val_loss: 0.7198\n",
            "Epoch 69/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6780 - loss: 0.5950 - val_accuracy: 0.5573 - val_loss: 0.7069\n",
            "Epoch 70/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6766 - loss: 0.5957 - val_accuracy: 0.5823 - val_loss: 0.6918\n",
            "Epoch 71/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6866 - loss: 0.5882 - val_accuracy: 0.5792 - val_loss: 0.6960\n",
            "Epoch 72/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6910 - loss: 0.5821 - val_accuracy: 0.5625 - val_loss: 0.7338\n",
            "Epoch 73/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6890 - loss: 0.5854 - val_accuracy: 0.5760 - val_loss: 0.7094\n",
            "Epoch 74/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6860 - loss: 0.5862 - val_accuracy: 0.5771 - val_loss: 0.6922\n",
            "Epoch 75/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6914 - loss: 0.5766 - val_accuracy: 0.5708 - val_loss: 0.6948\n",
            "Epoch 76/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6980 - loss: 0.5806 - val_accuracy: 0.5604 - val_loss: 0.7221\n",
            "Epoch 77/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6785 - loss: 0.5935 - val_accuracy: 0.5865 - val_loss: 0.6980\n",
            "Epoch 78/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6936 - loss: 0.5799 - val_accuracy: 0.5938 - val_loss: 0.6977\n",
            "Epoch 79/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7019 - loss: 0.5736 - val_accuracy: 0.5646 - val_loss: 0.7223\n",
            "Epoch 80/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6912 - loss: 0.5832 - val_accuracy: 0.5656 - val_loss: 0.7611\n",
            "Epoch 81/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6997 - loss: 0.5720 - val_accuracy: 0.5708 - val_loss: 0.7238\n",
            "Epoch 82/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6884 - loss: 0.5798 - val_accuracy: 0.5750 - val_loss: 0.7025\n",
            "Epoch 83/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6895 - loss: 0.5806 - val_accuracy: 0.5760 - val_loss: 0.7046\n",
            "Epoch 84/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6932 - loss: 0.5791 - val_accuracy: 0.5760 - val_loss: 0.7041\n",
            "Epoch 85/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6869 - loss: 0.5814 - val_accuracy: 0.5823 - val_loss: 0.7071\n",
            "Epoch 86/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7061 - loss: 0.5706 - val_accuracy: 0.5729 - val_loss: 0.7267\n",
            "Epoch 87/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6845 - loss: 0.5895 - val_accuracy: 0.5771 - val_loss: 0.7168\n",
            "Epoch 88/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6934 - loss: 0.5724 - val_accuracy: 0.5688 - val_loss: 0.7401\n",
            "Epoch 89/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7014 - loss: 0.5656 - val_accuracy: 0.5833 - val_loss: 0.7013\n",
            "Epoch 90/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7105 - loss: 0.5585 - val_accuracy: 0.5719 - val_loss: 0.7030\n",
            "Epoch 91/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6954 - loss: 0.5732 - val_accuracy: 0.5708 - val_loss: 0.7077\n",
            "Epoch 92/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7049 - loss: 0.5658 - val_accuracy: 0.5781 - val_loss: 0.7196\n",
            "Epoch 93/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7078 - loss: 0.5633 - val_accuracy: 0.5604 - val_loss: 0.7158\n",
            "Epoch 94/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7031 - loss: 0.5690 - val_accuracy: 0.5667 - val_loss: 0.7490\n",
            "Epoch 95/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7000 - loss: 0.5677 - val_accuracy: 0.5656 - val_loss: 0.7028\n",
            "Epoch 96/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7058 - loss: 0.5658 - val_accuracy: 0.5719 - val_loss: 0.7068\n",
            "Epoch 97/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7113 - loss: 0.5654 - val_accuracy: 0.5719 - val_loss: 0.7115\n",
            "Epoch 98/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6927 - loss: 0.5746 - val_accuracy: 0.5625 - val_loss: 0.7082\n",
            "Epoch 99/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7024 - loss: 0.5620 - val_accuracy: 0.5583 - val_loss: 0.7113\n",
            "Epoch 100/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6937 - loss: 0.5809 - val_accuracy: 0.5740 - val_loss: 0.7102\n",
            "Epoch 101/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7029 - loss: 0.5660 - val_accuracy: 0.5760 - val_loss: 0.7170\n",
            "Epoch 102/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7006 - loss: 0.5698 - val_accuracy: 0.5802 - val_loss: 0.7092\n",
            "Epoch 103/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7080 - loss: 0.5589 - val_accuracy: 0.5875 - val_loss: 0.7070\n",
            "Epoch 104/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7112 - loss: 0.5589 - val_accuracy: 0.5594 - val_loss: 0.7772\n",
            "Epoch 105/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7083 - loss: 0.5622 - val_accuracy: 0.5677 - val_loss: 0.7134\n",
            "Epoch 106/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7133 - loss: 0.5551 - val_accuracy: 0.5854 - val_loss: 0.7111\n",
            "Epoch 107/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7235 - loss: 0.5511 - val_accuracy: 0.5802 - val_loss: 0.7257\n",
            "Epoch 108/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7035 - loss: 0.5659 - val_accuracy: 0.5635 - val_loss: 0.7206\n",
            "Epoch 109/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7249 - loss: 0.5454 - val_accuracy: 0.5698 - val_loss: 0.7212\n",
            "Epoch 110/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7250 - loss: 0.5474 - val_accuracy: 0.5781 - val_loss: 0.7241\n",
            "Epoch 111/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7228 - loss: 0.5499 - val_accuracy: 0.5688 - val_loss: 0.7240\n",
            "Epoch 112/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7139 - loss: 0.5558 - val_accuracy: 0.5635 - val_loss: 0.8223\n",
            "Epoch 113/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7137 - loss: 0.5563 - val_accuracy: 0.5698 - val_loss: 0.7142\n",
            "Epoch 114/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7067 - loss: 0.5634 - val_accuracy: 0.5729 - val_loss: 0.7145\n",
            "Epoch 115/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7214 - loss: 0.5459 - val_accuracy: 0.5719 - val_loss: 0.7279\n",
            "Epoch 116/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7013 - loss: 0.5669 - val_accuracy: 0.5750 - val_loss: 0.7483\n",
            "Epoch 117/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7116 - loss: 0.5536 - val_accuracy: 0.5729 - val_loss: 0.7326\n",
            "Epoch 118/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7199 - loss: 0.5472 - val_accuracy: 0.5667 - val_loss: 0.7141\n",
            "Epoch 119/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7101 - loss: 0.5492 - val_accuracy: 0.5844 - val_loss: 0.7456\n",
            "Epoch 120/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6981 - loss: 0.5661 - val_accuracy: 0.5802 - val_loss: 0.7295\n",
            "Epoch 121/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7191 - loss: 0.5490 - val_accuracy: 0.5656 - val_loss: 0.7510\n",
            "Epoch 122/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7178 - loss: 0.5485 - val_accuracy: 0.5792 - val_loss: 0.7218\n",
            "Epoch 123/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7282 - loss: 0.5459 - val_accuracy: 0.5781 - val_loss: 0.7256\n",
            "Epoch 124/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7119 - loss: 0.5538 - val_accuracy: 0.5823 - val_loss: 0.7252\n",
            "Epoch 125/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7234 - loss: 0.5454 - val_accuracy: 0.5688 - val_loss: 0.7503\n",
            "Epoch 126/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7194 - loss: 0.5462 - val_accuracy: 0.5688 - val_loss: 0.7219\n",
            "Epoch 127/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7294 - loss: 0.5421 - val_accuracy: 0.5729 - val_loss: 0.7406\n",
            "Epoch 128/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7200 - loss: 0.5448 - val_accuracy: 0.5688 - val_loss: 0.7370\n",
            "Epoch 129/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7268 - loss: 0.5453 - val_accuracy: 0.5677 - val_loss: 0.7706\n",
            "Epoch 130/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7180 - loss: 0.5520 - val_accuracy: 0.5760 - val_loss: 0.7666\n",
            "Epoch 131/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7306 - loss: 0.5437 - val_accuracy: 0.5708 - val_loss: 0.7225\n",
            "Epoch 132/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7197 - loss: 0.5503 - val_accuracy: 0.5677 - val_loss: 0.7222\n",
            "Epoch 133/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7195 - loss: 0.5528 - val_accuracy: 0.5708 - val_loss: 0.7753\n",
            "Epoch 134/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7247 - loss: 0.5382 - val_accuracy: 0.5802 - val_loss: 0.7579\n",
            "Epoch 135/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7323 - loss: 0.5314 - val_accuracy: 0.5844 - val_loss: 0.7529\n",
            "Epoch 136/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7265 - loss: 0.5386 - val_accuracy: 0.5688 - val_loss: 0.7790\n",
            "Epoch 137/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7272 - loss: 0.5306 - val_accuracy: 0.5781 - val_loss: 0.7291\n",
            "Epoch 138/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7270 - loss: 0.5411 - val_accuracy: 0.5823 - val_loss: 0.7813\n",
            "Epoch 139/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7175 - loss: 0.5418 - val_accuracy: 0.5583 - val_loss: 0.7267\n",
            "Epoch 140/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7257 - loss: 0.5403 - val_accuracy: 0.5573 - val_loss: 0.7713\n",
            "Epoch 141/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7356 - loss: 0.5267 - val_accuracy: 0.5896 - val_loss: 0.7315\n",
            "Epoch 142/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7375 - loss: 0.5240 - val_accuracy: 0.5823 - val_loss: 0.7498\n",
            "Epoch 143/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7268 - loss: 0.5389 - val_accuracy: 0.5719 - val_loss: 0.7353\n",
            "Epoch 144/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7394 - loss: 0.5238 - val_accuracy: 0.5854 - val_loss: 0.7480\n",
            "Epoch 145/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7209 - loss: 0.5442 - val_accuracy: 0.5781 - val_loss: 0.7361\n",
            "Epoch 146/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7356 - loss: 0.5281 - val_accuracy: 0.5792 - val_loss: 0.7307\n",
            "Epoch 147/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7258 - loss: 0.5371 - val_accuracy: 0.5740 - val_loss: 0.7327\n",
            "Epoch 148/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7184 - loss: 0.5424 - val_accuracy: 0.5698 - val_loss: 0.7452\n",
            "Epoch 149/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7325 - loss: 0.5284 - val_accuracy: 0.5792 - val_loss: 0.7318\n",
            "Epoch 150/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7354 - loss: 0.5242 - val_accuracy: 0.5813 - val_loss: 0.7461\n",
            "Epoch 151/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7332 - loss: 0.5313 - val_accuracy: 0.5667 - val_loss: 0.8074\n",
            "Epoch 152/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7165 - loss: 0.5439 - val_accuracy: 0.5698 - val_loss: 0.7323\n",
            "Epoch 153/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7173 - loss: 0.5463 - val_accuracy: 0.5854 - val_loss: 0.7331\n",
            "Epoch 154/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7379 - loss: 0.5251 - val_accuracy: 0.5719 - val_loss: 0.7566\n",
            "Epoch 155/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7388 - loss: 0.5294 - val_accuracy: 0.5781 - val_loss: 0.7506\n",
            "Epoch 156/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7492 - loss: 0.5173 - val_accuracy: 0.5708 - val_loss: 0.7690\n",
            "Epoch 157/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7353 - loss: 0.5268 - val_accuracy: 0.5667 - val_loss: 0.7461\n",
            "Epoch 158/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7292 - loss: 0.5269 - val_accuracy: 0.5771 - val_loss: 0.7566\n",
            "Epoch 159/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7508 - loss: 0.5144 - val_accuracy: 0.5823 - val_loss: 0.7708\n",
            "Epoch 160/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7341 - loss: 0.5248 - val_accuracy: 0.5781 - val_loss: 0.7347\n",
            "Epoch 161/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7439 - loss: 0.5188 - val_accuracy: 0.5677 - val_loss: 0.7616\n",
            "Epoch 162/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7294 - loss: 0.5233 - val_accuracy: 0.5844 - val_loss: 0.7460\n",
            "Epoch 163/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7486 - loss: 0.5157 - val_accuracy: 0.5823 - val_loss: 0.7491\n",
            "Epoch 164/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7409 - loss: 0.5208 - val_accuracy: 0.5917 - val_loss: 0.7441\n",
            "Epoch 165/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7374 - loss: 0.5263 - val_accuracy: 0.5823 - val_loss: 0.7418\n",
            "Epoch 166/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7319 - loss: 0.5267 - val_accuracy: 0.5875 - val_loss: 0.7558\n",
            "Epoch 167/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7243 - loss: 0.5288 - val_accuracy: 0.5813 - val_loss: 0.7537\n",
            "Epoch 168/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7354 - loss: 0.5188 - val_accuracy: 0.5781 - val_loss: 0.7428\n",
            "Epoch 169/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7417 - loss: 0.5195 - val_accuracy: 0.5875 - val_loss: 0.7654\n",
            "Epoch 170/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7473 - loss: 0.5168 - val_accuracy: 0.5875 - val_loss: 0.7521\n",
            "Epoch 171/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7506 - loss: 0.5068 - val_accuracy: 0.5865 - val_loss: 0.7677\n",
            "Epoch 172/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7449 - loss: 0.5168 - val_accuracy: 0.5698 - val_loss: 0.7630\n",
            "Epoch 173/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7362 - loss: 0.5200 - val_accuracy: 0.5781 - val_loss: 0.7447\n",
            "Epoch 174/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7432 - loss: 0.5165 - val_accuracy: 0.5667 - val_loss: 0.7970\n",
            "Epoch 175/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7383 - loss: 0.5205 - val_accuracy: 0.5854 - val_loss: 0.7484\n",
            "Epoch 176/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7357 - loss: 0.5216 - val_accuracy: 0.5740 - val_loss: 0.7439\n",
            "Epoch 177/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5192 - val_accuracy: 0.5896 - val_loss: 0.7522\n",
            "Epoch 178/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7524 - loss: 0.5069 - val_accuracy: 0.5854 - val_loss: 0.7658\n",
            "Epoch 179/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7339 - loss: 0.5319 - val_accuracy: 0.5802 - val_loss: 0.7429\n",
            "Epoch 180/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7494 - loss: 0.5042 - val_accuracy: 0.5823 - val_loss: 0.7483\n",
            "Epoch 181/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7547 - loss: 0.5008 - val_accuracy: 0.5729 - val_loss: 0.7685\n",
            "Epoch 182/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7426 - loss: 0.5127 - val_accuracy: 0.5823 - val_loss: 0.7497\n",
            "Epoch 183/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7511 - loss: 0.5072 - val_accuracy: 0.5833 - val_loss: 0.7550\n",
            "Epoch 184/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7441 - loss: 0.5068 - val_accuracy: 0.5719 - val_loss: 0.7582\n",
            "Epoch 185/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7422 - loss: 0.5135 - val_accuracy: 0.5792 - val_loss: 0.7664\n",
            "Epoch 186/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7416 - loss: 0.5181 - val_accuracy: 0.5781 - val_loss: 0.7552\n",
            "Epoch 187/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7561 - loss: 0.5009 - val_accuracy: 0.5958 - val_loss: 0.7812\n",
            "Epoch 188/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7431 - loss: 0.5109 - val_accuracy: 0.5781 - val_loss: 0.7585\n",
            "Epoch 189/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7581 - loss: 0.5023 - val_accuracy: 0.5792 - val_loss: 0.7557\n",
            "Epoch 190/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7464 - loss: 0.5097 - val_accuracy: 0.5760 - val_loss: 0.7553\n",
            "Epoch 191/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7486 - loss: 0.5093 - val_accuracy: 0.5854 - val_loss: 0.7527\n",
            "Epoch 192/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7560 - loss: 0.4969 - val_accuracy: 0.5865 - val_loss: 0.7518\n",
            "Epoch 193/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7494 - loss: 0.5051 - val_accuracy: 0.5688 - val_loss: 0.7981\n",
            "Epoch 194/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7563 - loss: 0.4922 - val_accuracy: 0.5667 - val_loss: 0.7583\n",
            "Epoch 195/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7566 - loss: 0.4973 - val_accuracy: 0.5750 - val_loss: 0.7769\n",
            "Epoch 196/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7572 - loss: 0.5101 - val_accuracy: 0.5771 - val_loss: 0.7672\n",
            "Epoch 197/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7578 - loss: 0.4955 - val_accuracy: 0.5677 - val_loss: 0.7681\n",
            "Epoch 198/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7553 - loss: 0.5022 - val_accuracy: 0.5698 - val_loss: 0.7588\n",
            "Epoch 199/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7555 - loss: 0.5029 - val_accuracy: 0.5896 - val_loss: 0.7650\n",
            "Epoch 200/200\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7506 - loss: 0.4998 - val_accuracy: 0.5875 - val_loss: 0.7638\n",
            "Training complete.\n",
            "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Test Accuracy: 0.5933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"  # Path to the positive data file\n",
        "negative_data_file = \"rt-polarity.neg\"  # Path to the negative data file\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 100  # Adjusted embedding dimension to match Word2Vec\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text, y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
        "\n",
        "# Tokenization\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "\n",
        "# Train the Word2Vec model\n",
        "w2v_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    conv_layer = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu')(embedding_layer)\n",
        "    maxpool_layer = tf.keras.layers.MaxPooling1D(pool_size=max_length - 3 + 1)(conv_layer)\n",
        "    flatten_layer = tf.keras.layers.Flatten()(maxpool_layer)\n",
        "    output_layer = tf.keras.layers.Dense(2, activation='softmax')(flatten_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Ensure the model is built by calling it on dummy data\n",
        "dummy_input = np.zeros((1, max_length))  # Dummy input with shape (1, max_length)\n",
        "textcnn_model(dummy_input)  # Call the model to build it\n",
        "\n",
        "# Train the TextCNN model\n",
        "history = textcnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs\n",
        ")\n",
        "\n",
        "# Ensure model training is completed before feature extraction\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Create a feature extraction model to get the outputs of the Flatten layer\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "X_train_features = feature_extraction_model.predict(X_train)\n",
        "X_test_features = feature_extraction_model.predict(X_test)\n",
        "\n",
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression(penalty='l2', C=0.8)\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/\n"
      ],
      "metadata": {
        "id": "IhPP1n4ttH1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e1b689-4cff-4bb6-9767-8e238c1c69b1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  rt-polarity.neg\trt-polarity.pos  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1kR-kRKtIrP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DObGGxmrXnHP",
        "outputId": "2e1acf34-3ddb-4737-a917-8661e5811da6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5998\n",
            "Train Accuracy: 0.7730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=85, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF_RZ2diXoN1",
        "outputId": "28339a41-2a7c-4c65-c263-46046e72542a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5951\n",
            "Train Accuracy: 0.9991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKGRlntma92P",
        "outputId": "e271afed-f3d3-44a0-e362-17ac6f1381e6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5961\n",
            "Train Accuracy: 0.7778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AS svm is working betterly so testing it on realtime human input"
      ],
      "metadata": {
        "id": "fdq9FlW7HfMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual sentence input and prediction\n",
        "def predict_sentence(sentence):\n",
        "    # Preprocess the input sentence\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sentence = pad_sequences(tokenized_sentence, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Extract features using the feature extraction model\n",
        "    sentence_features = feature_extraction_model.predict(padded_sentence)  # Only one input\n",
        "\n",
        "    # Predict using the SVM model\n",
        "    prediction = svm_model.predict(sentence_features)\n",
        "\n",
        "    return \"Positive\" if prediction[0] == 1 else \"Negative\"\n",
        "\n",
        "# Example usage: input your sentence here\n",
        "input_sentence = input(\"Enter a sentence for sentiment prediction: \")\n",
        "result = predict_sentence(input_sentence)\n",
        "print(f\"Sentiment Prediction: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD-NSStWHb9l",
        "outputId": "ec301a31-26f3-4d1a-a2dc-36965ed5dda8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for sentiment prediction: i m bad\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
            "Sentiment Prediction: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 layer structure : 1. trainable=false 2. trainable=true"
      ],
      "metadata": {
        "id": "Bnv79jNnGK4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"\n",
        "negative_data_file = \"rt-polarity.neg\"\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 19\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text, y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
        "\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "print(\"adhi\", len(X_train))\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "print(\"adhi1\", len(X_train))\n",
        "\n",
        "# Train the Word2Vec model\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    # Define inputs for each channel\n",
        "    input_static = tf.keras.layers.Input(shape=(max_length,))\n",
        "    input_non_static = tf.keras.layers.Input(shape=(max_length,))\n",
        "\n",
        "    # Static channel: Use pre-trained embedding matrix and set trainable=False\n",
        "    embedding_static = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], input_length=max_length, trainable=False)(input_static)\n",
        "\n",
        "    # Non-static channel: Use separate embedding layer with trainable=True\n",
        "    embedding_non_static = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], input_length=max_length, trainable=True)(input_non_static)\n",
        "\n",
        "    # Convolution and max-pooling layers for both channels\n",
        "    conv_static = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu')(embedding_static)\n",
        "    conv_non_static = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu')(embedding_non_static)\n",
        "\n",
        "    maxpool_static = tf.keras.layers.GlobalMaxPooling1D()(conv_static)\n",
        "    maxpool_non_static = tf.keras.layers.GlobalMaxPooling1D()(conv_non_static)\n",
        "\n",
        "    # Concatenate the outputs of both channels\n",
        "    merged = tf.keras.layers.Concatenate()([maxpool_static, maxpool_non_static])\n",
        "\n",
        "    # Dropout layer for regularization\n",
        "    dropout = tf.keras.layers.Dropout(0.7)(merged)\n",
        "\n",
        "    # Output layer\n",
        "    output = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "    # Define the model with two inputs and one output\n",
        "    model = tf.keras.Model(inputs=[input_static, input_non_static], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the TextCNN model without early stopping\n",
        "history = textcnn_model.fit([X_train, X_train], y_train,\n",
        "                            validation_data=([X_val, X_val], y_val),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=num_epochs)\n",
        "\n",
        "# Remove the last layers of the TextCNN model\n",
        "for layer in textcnn_model.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a new model that outputs the Flatten layer's activations\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "X_train_features = feature_extraction_model.predict([X_train, X_train])\n",
        "X_test_features = feature_extraction_model.predict([X_test, X_test])\n",
        "\n",
        "print(X_train_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZUMGCoLT8w",
        "outputId": "aeb1d6cf-bf88-4f3f-abef-bc17a4d9bb2a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "8635 1067 8635 1067\n",
            "adhi 8635\n",
            "adhi1 8635\n",
            "Epoch 1/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.5074 - loss: 0.7275 - val_accuracy: 0.6260 - val_loss: 0.6680\n",
            "Epoch 2/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6658 - loss: 0.6313 - val_accuracy: 0.7417 - val_loss: 0.5413\n",
            "Epoch 3/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8354 - loss: 0.4113 - val_accuracy: 0.7719 - val_loss: 0.4796\n",
            "Epoch 4/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9173 - loss: 0.2426 - val_accuracy: 0.7750 - val_loss: 0.5485\n",
            "Epoch 5/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9634 - loss: 0.1247 - val_accuracy: 0.7542 - val_loss: 0.6760\n",
            "Epoch 6/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9821 - loss: 0.0667 - val_accuracy: 0.7521 - val_loss: 0.7793\n",
            "Epoch 7/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0439 - val_accuracy: 0.7500 - val_loss: 0.8944\n",
            "Epoch 8/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9943 - loss: 0.0247 - val_accuracy: 0.7385 - val_loss: 0.9805\n",
            "Epoch 9/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9938 - loss: 0.0230 - val_accuracy: 0.7427 - val_loss: 1.0762\n",
            "Epoch 10/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9959 - loss: 0.0153 - val_accuracy: 0.7396 - val_loss: 1.1473\n",
            "Epoch 11/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9974 - loss: 0.0105 - val_accuracy: 0.7312 - val_loss: 1.1999\n",
            "Epoch 12/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9966 - loss: 0.0116 - val_accuracy: 0.7292 - val_loss: 1.2853\n",
            "Epoch 13/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9981 - loss: 0.0081 - val_accuracy: 0.7323 - val_loss: 1.3369\n",
            "Epoch 14/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9990 - loss: 0.0065 - val_accuracy: 0.7365 - val_loss: 1.4150\n",
            "Epoch 15/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9989 - loss: 0.0062 - val_accuracy: 0.7427 - val_loss: 1.4608\n",
            "Epoch 16/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9988 - loss: 0.0074 - val_accuracy: 0.7312 - val_loss: 1.5004\n",
            "Epoch 17/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0055 - val_accuracy: 0.7323 - val_loss: 1.5371\n",
            "Epoch 18/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0072 - val_accuracy: 0.7302 - val_loss: 1.5698\n",
            "Epoch 19/19\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9979 - loss: 0.0066 - val_accuracy: 0.7354 - val_loss: 1.6336\n",
            "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "[[0.         0.         0.23713523 ... 0.         0.25889093 0.35966703]\n",
            " [0.         0.         0.46184462 ... 0.5237468  0.11034349 0.26148343]\n",
            " [0.         0.         0.5027038  ... 0.05422801 0.48828474 0.59221727]\n",
            " ...\n",
            " [0.         0.         0.42353776 ... 0.7553021  0.00367936 0.04307306]\n",
            " [0.         0.         0.41661212 ... 0.04832137 0.33773735 0.39125356]\n",
            " [0.         0.         0.37320018 ... 0.         0.11508954 0.30545956]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGxDWeiFzYK9",
        "outputId": "32f358db-060d-41ac-e21d-f09d769036ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8635"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUAn-muIW1w1",
        "outputId": "b36841b2-c555-4590-8e06-6a787af3aac5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7104\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "juKSWgHRvG97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVfDrrDvR3IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=90,max_depth=5, min_samples_split=4, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VCQVAxPx9mn",
        "outputId": "30261e2a-b1ef-4b43-c9af-d6cbd26ea708"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7048\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='linear', C=0.7,gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "4ogElg7Swfol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf',gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "gu_VgBFo8IL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Instantiate the SVM classifier\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_features, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Instantiate the SVM model with the best parameters\n",
        "best_svm_model = SVC(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "accuracy_test = best_svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "accuracy_train = best_svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "id": "--cameTb8Ogi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize kNN classifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train kNN model\n",
        "knn_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate kNN model\n",
        "accuracy_test = knn_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "\n",
        "# Evaluate kNN model on the training data\n",
        "accuracy_train = knn_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "id": "llBdlLJVr1M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 layer with trainable=true**"
      ],
      "metadata": {
        "id": "4QHJmAYrGy_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"  # Path to the positive data file\n",
        "negative_data_file = \"rt-polarity.neg\"  # Path to the negative data file\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 100  # Adjusted embedding dimension to match Word2Vec\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text, y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
        "\n",
        "# Tokenization\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "\n",
        "# Train the Word2Vec model\n",
        "w2v_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True)(input_layer)\n",
        "    conv_layer = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu')(embedding_layer)\n",
        "    maxpool_layer = tf.keras.layers.MaxPooling1D(pool_size=max_length - 3 + 1)(conv_layer)\n",
        "    flatten_layer = tf.keras.layers.Flatten()(maxpool_layer)\n",
        "    output_layer = tf.keras.layers.Dense(2, activation='softmax')(flatten_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Ensure the model is built by calling it on dummy data\n",
        "dummy_input = np.zeros((1, max_length))  # Dummy input with shape (1, max_length)\n",
        "textcnn_model(dummy_input)  # Call the model to build it\n",
        "\n",
        "# Train the TextCNN model\n",
        "history = textcnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs\n",
        ")\n",
        "\n",
        "# Ensure model training is completed before feature extraction\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Create a feature extraction model to get the outputs of the Flatten layer\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "X_train_features = feature_extraction_model.predict(X_train)\n",
        "X_test_features = feature_extraction_model.predict(X_test)\n",
        "\n",
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression(penalty='l2', C=0.8)\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Z7YT6KKmG47e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "id": "6kmyB4QpHQ3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=85, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "id": "NKJPnUyRHRf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "w9tqXi24HWAK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}