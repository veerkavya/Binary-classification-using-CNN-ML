{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1VpIC5MN2o8",
        "outputId": "39e88c9a-9454-4afe-9590-96320860579d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "hE8uZXiUMv4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n"
      ],
      "metadata": {
        "id": "b2Cg99nqzNR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OTV8kallEW17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define TensorBoard callback\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
        "\n",
        "# Define ModelCheckpoint callback to save the model\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoint.h5\", save_best_only=True, save_weights_only=False)\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "9mqMxi1xOPZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agvH5Ry0xr3U",
        "outputId": "9aa15f76-5f09-4dbe-923f-cde53e0f5c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "8635 1067 8635 1067\n",
            "adhi 8635\n",
            "adhi1 8635\n",
            "Epoch 1/200\n",
            "135/135 [==============================] - 9s 57ms/step - loss: 0.6997 - accuracy: 0.5304 - val_loss: 0.6945 - val_accuracy: 0.5271\n",
            "Epoch 2/200\n",
            "135/135 [==============================] - 8s 61ms/step - loss: 0.6839 - accuracy: 0.5538 - val_loss: 0.6785 - val_accuracy: 0.5688\n",
            "Epoch 3/200\n",
            "135/135 [==============================] - 8s 59ms/step - loss: 0.6810 - accuracy: 0.5639 - val_loss: 0.6794 - val_accuracy: 0.5729\n",
            "Epoch 4/200\n",
            "135/135 [==============================] - 6s 41ms/step - loss: 0.6779 - accuracy: 0.5728 - val_loss: 0.6789 - val_accuracy: 0.5667\n",
            "Epoch 5/200\n",
            "135/135 [==============================] - 3s 25ms/step - loss: 0.6759 - accuracy: 0.5749 - val_loss: 0.6794 - val_accuracy: 0.5583\n",
            "Epoch 6/200\n",
            "135/135 [==============================] - 3s 25ms/step - loss: 0.6741 - accuracy: 0.5814 - val_loss: 0.6736 - val_accuracy: 0.5958\n",
            "Epoch 7/200\n",
            "135/135 [==============================] - 5s 38ms/step - loss: 0.6763 - accuracy: 0.5758 - val_loss: 0.6880 - val_accuracy: 0.5365\n",
            "Epoch 8/200\n",
            "135/135 [==============================] - 3s 25ms/step - loss: 0.6707 - accuracy: 0.5844 - val_loss: 0.6834 - val_accuracy: 0.5635\n",
            "Epoch 9/200\n",
            "135/135 [==============================] - 4s 26ms/step - loss: 0.6700 - accuracy: 0.5880 - val_loss: 0.6837 - val_accuracy: 0.5615\n",
            "Epoch 10/200\n",
            "135/135 [==============================] - 5s 36ms/step - loss: 0.6641 - accuracy: 0.5980 - val_loss: 0.6848 - val_accuracy: 0.5583\n",
            "Epoch 11/200\n",
            "135/135 [==============================] - 4s 27ms/step - loss: 0.6642 - accuracy: 0.5987 - val_loss: 0.6722 - val_accuracy: 0.5833\n",
            "Epoch 11: early stopping\n",
            "270/270 [==============================] - 2s 6ms/step\n",
            "34/34 [==============================] - 0s 6ms/step\n",
            "[[0.         0.         0.03629401 ... 0.         0.41685843 0.        ]\n",
            " [0.         0.         0.37316272 ... 0.49584827 0.8770889  0.6740466 ]\n",
            " [0.         0.         0.05944011 ... 0.02150325 1.406523   0.        ]\n",
            " ...\n",
            " [0.         0.         0.11569041 ... 0.         1.0854131  0.22821611]\n",
            " [0.         0.         0.0540434  ... 0.25144842 1.2361215  0.13470054]\n",
            " [0.         0.         0.03674773 ... 0.         1.1144781  0.        ]]\n",
            "Test Accuracy: 0.6054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"\n",
        "negative_data_file = \"rt-polarity.neg\"\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text,y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
        "\n",
        "\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "print(\"adhi\",len(X_train))\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "print(\"adhi1\",len(X_train))\n",
        "# Train the Word2Vec model\n",
        "\n",
        "\n",
        "w2v_model = Word2Vec(sentences,vector_size=100, window=5, min_count=5, workers=4)\n",
        "#train=[[0 0 0 0  2 3 4 ]]\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(pool_size=embedding_matrix.shape[1] - 3 + 1))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))  # Softmax layer for classification\n",
        "    return model\n",
        "\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the TextCNN model\n",
        "history = textcnn_model.fit(X_train, y_train,\n",
        "                             validation_data=(X_val, y_val),\n",
        "                             batch_size=batch_size,\n",
        "                             epochs=num_epochs, callbacks=[early_stopping_callback]\n",
        "                            )\n",
        "\n",
        "# Remove the last layers of the TextCNN model\n",
        "for layer in textcnn_model.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a new model that outputs the Flatten layer's activations\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "X_train_features = feature_extraction_model.predict(X_train)\n",
        "X_test_features = feature_extraction_model.predict(X_test)\n",
        "\n",
        "print(X_train_features)\n",
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression(penalty='l2', C=0.8)\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhPP1n4ttH1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1kR-kRKtIrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DObGGxmrXnHP",
        "outputId": "dbcc5964-be09-4049-8745-2fc7bd09c843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6054\n",
            "Train Accuracy: 0.6321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=85, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF_RZ2diXoN1",
        "outputId": "ac008bb0-fce5-4247-a1df-f79e388ae31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5979\n",
            "Train Accuracy: 0.9990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKGRlntma92P",
        "outputId": "415d16c8-c1cb-4564-c66e-fa1e797cc512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6045\n",
            "Train Accuracy: 0.6412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"\n",
        "negative_data_file = \"rt-polarity.neg\"\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 19\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text,y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
        "\n",
        "\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "print(\"adhi\",len(X_train))\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "print(\"adhi1\",len(X_train))\n",
        "# Train the Word2Vec model\n",
        "\n",
        "\n",
        "w2v_model = Word2Vec(sentences,vector_size=100, window=5, min_count=5, workers=4)\n",
        "#train=[[0 0 0 0  2 3 4 ]]\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    # Define inputs for each channel\n",
        "    input_static = tf.keras.layers.Input(shape=(max_length,))\n",
        "    input_non_static = tf.keras.layers.Input(shape=(max_length,))\n",
        "\n",
        "    # Static channel: Use pre-trained embedding matrix and set trainable=False\n",
        "    embedding_static = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], input_length=max_length, trainable=False)(input_static)\n",
        "\n",
        "    # Non-static channel: Use separate embedding layer with trainable=True\n",
        "    embedding_non_static = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], input_length=max_length, trainable=True)(input_non_static)\n",
        "\n",
        "    # Convolution and max-pooling layers for both channels\n",
        "    conv_static = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu')(embedding_static)\n",
        "    conv_non_static = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu')(embedding_non_static)\n",
        "\n",
        "    maxpool_static = tf.keras.layers.GlobalMaxPooling1D()(conv_static)\n",
        "    maxpool_non_static = tf.keras.layers.GlobalMaxPooling1D()(conv_non_static)\n",
        "\n",
        "    # Concatenate the outputs of both channels\n",
        "    merged = tf.keras.layers.Concatenate()([maxpool_static, maxpool_non_static])\n",
        "\n",
        "    # Dropout layer for regularization\n",
        "    dropout = tf.keras.layers.Dropout(0.7)(merged)\n",
        "\n",
        "    # Output layer\n",
        "    output = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "    # Define the model with two inputs and one output\n",
        "    model = tf.keras.Model(inputs=[input_static, input_non_static], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Assuming X_train_static and X_train_non_static are the input data for static and non-static channels, respectively\n",
        "history = textcnn_model.fit([X_train, X_train], y_train,\n",
        "                            validation_data=([X_val, X_val], y_val),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=num_epochs,\n",
        "                            callbacks=[early_stopping_callback])\n",
        "\n",
        "\n",
        "# Remove the last layers of the TextCNN model\n",
        "for layer in textcnn_model.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a new model that outputs the Flatten layer's activations\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "\n",
        "X_train_features= feature_extraction_model.predict([X_train, X_train])\n",
        "\n",
        "X_test_features = feature_extraction_model.predict([X_test,X_test])\n",
        "\n",
        "print(X_train_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZUMGCoLT8w",
        "outputId": "c0665955-5fd8-4443-be1f-aa576eb543e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "8635 1067 8635 1067\n",
            "adhi 8635\n",
            "adhi1 8635\n",
            "Epoch 1/19\n",
            "135/135 [==============================] - 18s 129ms/step - loss: 0.7282 - accuracy: 0.5188 - val_loss: 0.6848 - val_accuracy: 0.5531\n",
            "Epoch 2/19\n",
            "135/135 [==============================] - 18s 132ms/step - loss: 0.7028 - accuracy: 0.5246 - val_loss: 0.6816 - val_accuracy: 0.5729\n",
            "Epoch 3/19\n",
            "135/135 [==============================] - 11s 82ms/step - loss: 0.6890 - accuracy: 0.5493 - val_loss: 0.6801 - val_accuracy: 0.5708\n",
            "Epoch 4/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.6773 - accuracy: 0.5715 - val_loss: 0.6779 - val_accuracy: 0.5865\n",
            "Epoch 5/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.6687 - accuracy: 0.6045 - val_loss: 0.6743 - val_accuracy: 0.5896\n",
            "Epoch 6/19\n",
            "135/135 [==============================] - 10s 77ms/step - loss: 0.6632 - accuracy: 0.6248 - val_loss: 0.6695 - val_accuracy: 0.6052\n",
            "Epoch 7/19\n",
            "135/135 [==============================] - 10s 75ms/step - loss: 0.6522 - accuracy: 0.6488 - val_loss: 0.6611 - val_accuracy: 0.6417\n",
            "Epoch 8/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.6343 - accuracy: 0.6864 - val_loss: 0.6511 - val_accuracy: 0.6604\n",
            "Epoch 9/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.6153 - accuracy: 0.7173 - val_loss: 0.6394 - val_accuracy: 0.6875\n",
            "Epoch 10/19\n",
            "135/135 [==============================] - 11s 84ms/step - loss: 0.5867 - accuracy: 0.7556 - val_loss: 0.6202 - val_accuracy: 0.6844\n",
            "Epoch 11/19\n",
            "135/135 [==============================] - 10s 74ms/step - loss: 0.5495 - accuracy: 0.7911 - val_loss: 0.5985 - val_accuracy: 0.7094\n",
            "Epoch 12/19\n",
            "135/135 [==============================] - 10s 77ms/step - loss: 0.5095 - accuracy: 0.8188 - val_loss: 0.5754 - val_accuracy: 0.7365\n",
            "Epoch 13/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.4625 - accuracy: 0.8455 - val_loss: 0.5472 - val_accuracy: 0.7333\n",
            "Epoch 14/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.4137 - accuracy: 0.8681 - val_loss: 0.5245 - val_accuracy: 0.7458\n",
            "Epoch 15/19\n",
            "135/135 [==============================] - 11s 79ms/step - loss: 0.3688 - accuracy: 0.8812 - val_loss: 0.5056 - val_accuracy: 0.7490\n",
            "Epoch 16/19\n",
            "135/135 [==============================] - 10s 72ms/step - loss: 0.3234 - accuracy: 0.9043 - val_loss: 0.4941 - val_accuracy: 0.7615\n",
            "Epoch 17/19\n",
            "135/135 [==============================] - 11s 80ms/step - loss: 0.2841 - accuracy: 0.9169 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
            "Epoch 18/19\n",
            "135/135 [==============================] - 12s 87ms/step - loss: 0.2505 - accuracy: 0.9299 - val_loss: 0.4886 - val_accuracy: 0.7563\n",
            "Epoch 19/19\n",
            "135/135 [==============================] - 11s 81ms/step - loss: 0.2219 - accuracy: 0.9369 - val_loss: 0.4949 - val_accuracy: 0.7573\n",
            "270/270 [==============================] - 4s 16ms/step\n",
            "34/34 [==============================] - 0s 10ms/step\n",
            "[[0.         0.         0.0070472  ... 0.         0.045905   0.14847575]\n",
            " [0.24889784 0.09403946 0.08881111 ... 0.127705   0.02563134 0.09331499]\n",
            " [0.04261424 0.         0.08713326 ... 0.03036214 0.13858238 0.16090941]\n",
            " ...\n",
            " [0.19902733 0.         0.21552314 ... 0.1698179  0.02474797 0.00896119]\n",
            " [0.09088397 0.         0.05239641 ... 0.07055436 0.08427837 0.1418003 ]\n",
            " [0.         0.         0.02859469 ... 0.08371648 0.10023514 0.13955782]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGxDWeiFzYK9",
        "outputId": "32f358db-060d-41ac-e21d-f09d769036ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8635"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUAn-muIW1w1",
        "outputId": "1a0ed8ef-c479-4f66-b0c7-55ab942db26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7451\n",
            "Train Accuracy: 0.9678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "juKSWgHRvG97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVfDrrDvR3IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=90,max_depth=5, min_samples_split=4, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VCQVAxPx9mn",
        "outputId": "1a3d6f20-b23c-474e-c508-7e2d23b48207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7535\n",
            "Train Accuracy: 0.9726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 4, 6],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Instantiate the random forest classifier\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_features, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Instantiate the random forest model with the best parameters\n",
        "best_random_forest_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "accuracy = best_random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "accuracy_train = best_random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVzuptuVmxRM",
        "outputId": "e9f34bdf-5aee-4867-d694-df1e0008ae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dy-_KvjSyHbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='linear', C=0.7,gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "4ogElg7Swfol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b45b0e-fcc8-4fba-cce8-d1de526a812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7488\n",
            "Train Accuracy: 0.9716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf',gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_VgBFo8IL3",
        "outputId": "7b507dfa-b5c9-4cb3-a1b2-4b8fbee8558c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7545\n",
            "Train Accuracy: 0.9701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Instantiate the SVM classifier\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_features, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Instantiate the SVM model with the best parameters\n",
        "best_svm_model = SVC(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "accuracy_test = best_svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "accuracy_train = best_svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "id": "--cameTb8Ogi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize kNN classifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train kNN model\n",
        "knn_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate kNN model\n",
        "accuracy_test = knn_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "\n",
        "# Evaluate kNN model on the training data\n",
        "accuracy_train = knn_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "id": "llBdlLJVr1M3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}